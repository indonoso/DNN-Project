data:
  dataset:
    train_path: drive/MyDrive/Colab Notebooks/DL/data/SQuAD/train-v2.0.json
    dev_path: drive/MyDrive/Colab Notebooks/DL/data/SQuAD/dev-v2.0.json
  dataset_h5: null.txt
  word_embedding_path: drive/MyDrive/Colab Notebooks/DL/data/GloVe/glove.6B.300d.txt
  kg_embeddings_vec_path: drive/MyDrive/Colab Notebooks/DL/data/FB15K-237.2/entity_embeddings_32.tsv
  kg_patterns2id_path: drive/MyDrive/Colab Notebooks/DL/data/FB15K-237.2/kg_patterns2id.json

  model_path: drive/MyDrive/Colab Notebooks/DL/data/checkpoint/2-model-weight
  checkpoint_path: drive/MyDrive/Colab Notebooks/DL/data/checkpoint/2-metrics

  processed:
    kg_embeddings_path: False
    pos_embeddings_path: False
    word_embeddings_path: drive/MyDrive/Colab Notebooks/DL/data/processed/word_embeddings
    meta_path: drive/MyDrive/Colab Notebooks/DL/data/processed/1-meta.pickle
    dataset_path: drive/MyDrive/Colab Notebooks/DL/data/processed/1-dataset.pickle

model:
  match_lstm_input_size: 100
  hidden_size: 30

global:
  random_seed: 123
  num_data_workers: 5   # for data loader

preprocess:
  word_embedding_size: 300
  kg_embedding_size: 32
  pos_embedding_size: 49
  ignore_max_len: 600 # in train data, context token len > ignore_max_len will be dropped
  use_char: False
  use_pos: False
  use_ent: False
  use_em: False
  use_kg: False
  use_em_lemma: False

train:
  batch_size: 32
  valid_batch_size: 32
  epoch: 30
  enable_cuda: True

  optimizer: 'adamax'  # adam, sgd, adamax, adadelta(default is adamax)
  learning_rate: 0.002  # only for sgd
  clip_grad_norm: 10

test:
  batch_size: 32
  enable_cuda: True